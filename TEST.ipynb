{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb4b384f-d8c1-4a7f-8560-5f3a99f8a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from gensim.utils import tokenize\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "data=load_dataset('imdb')\n",
    "device='cuda'\n",
    "from torch.nn import Module\n",
    "from torch import nn \n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a17466-6e1d-4801-a521-4d4bcf73d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:26<00:00, 960.39it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sentences=[]\n",
    "sent_threshold=32\n",
    "\n",
    "for text_block in tqdm(data['train']['text']):\n",
    "    for sentences in TextBlob(text_block).sentences:\n",
    "        len_of_sent=sentences.words \n",
    "        for sent in sentences.split('.<br /><br />'):\n",
    "            if len(len_of_sent)<sent_threshold:\n",
    "                all_sentences.append(sent)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "161c85e5-4ba3-4474-b0c6-e2ff368bd35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 227434/227434 [00:01<00:00, 124312.33it/s]\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "for sent in tqdm(all_sentences):\n",
    "    for word in tokenize(sent):\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "153a013e-f901-4805-9a6d-dd8e74dcd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr=Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb0b2108-594f-4c1c-be86-c22363276fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=40000\n",
    "our_words=set(['<unk>','<bos>','<eos>','<pad>'])\n",
    "for word,cnt in cntr.most_common()[0:vocab_size]:\n",
    "    our_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "455d81c4-3a06-4cd2-a3d0-63dbdb660c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40004"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(our_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66f4460d-4d65-4269-9b9a-ede65b5e0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "Am\n",
      "Curious\n",
      "Yellow\n",
      "is\n",
      "a\n",
      "risible\n",
      "and\n",
      "pretentious\n",
      "steaming\n",
      "pile\n"
     ]
    }
   ],
   "source": [
    "for i in tokenize(all_sentences[5]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ba803f6-e3d5-466a-a755-9b9bba2bb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int={word:cnt for cnt,word in enumerate(our_words)}\n",
    "int2word={cnt:word for cnt,word in enumerate(our_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50f8c7bf-6735-41eb-8264-e6f0cc1ef9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_dataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super(word_dataset,self).__init__()\n",
    "        self.data=data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        selected_sent=self.data[idx]\n",
    "        indexed_text=[word2int['<bos>']]\n",
    "        main_text=[word2int[word] if word in word2int else word2int['<unk>'] for word in tokenize(selected_sent.lower())]\n",
    "        indexed_text=indexed_text+main_text+[word2int['<eos>']]\n",
    "        return indexed_text\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d264972-1229-42f5-9c59-c9b0f1ad0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch):\n",
    "    lenghts=(len(x) for x in batch)\n",
    "    max_len=max(lenghts)\n",
    "\n",
    "    new_batch=[]\n",
    "    for sent in batch:\n",
    "        for pad in range(max_len-len(sent)):\n",
    "            sent.append(word2int['<pad>'])\n",
    "        new_batch.append(sent)\n",
    "\n",
    "    new_batch=torch.LongTensor(new_batch).to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'inp':new_batch[:,:-1],\n",
    "        'label':new_batch[:,1:]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afe11823-45b8-4842-b796-85419a4f09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=word_dataset(all_sentences)\n",
    "train_dataloader=DataLoader(train_data,batch_size=32,shuffle=True,collate_fn=make_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a56789a9-6de8-4780-81b5-8d2c5ea958df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(Module):\n",
    "    def __init__(self,vocab_size,hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb=nn.Embedding(vocab_size,hidden_size)\n",
    "        self.gru=nn.GRU(hidden_size,hidden_size,num_layers=3,batch_first=True)\n",
    "\n",
    "        self.lay_with_drop=nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Dropout(),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.final_lin=nn.Sequential(\n",
    "            nn.Linear(hidden_size,vocab_size)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self,text):\n",
    "        \n",
    "        emb_x=self.emb(text)\n",
    "        \n",
    "    \n",
    "        x,_=self.gru(emb_x)\n",
    "        \n",
    "\n",
    "        #agr_x=x.mean(dim=1)#???????\n",
    "        \n",
    "\n",
    "        x=self.lay_with_drop(x)\n",
    "        x=self.final_lin(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2907ad28-23d9-4509-bbfe-d4da828f0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LanguageModel(len(our_words),128).to(device)\n",
    "loss_fn=nn.CrossEntropyLoss(ignore_index=word2int['<pad>'])\n",
    "optimizer=AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a09b8c26-2b2e-4473-a395-56d248b94ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40004"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(our_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7fbd996-535b-4932-af88-ff7e7772778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,model,loss_fn,optimizer,dataloader):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in (pbar:=tqdm(dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            pred=model(batch['inp']).flatten(start_dim=0,end_dim=1)\n",
    "            \n",
    "            loss=loss_fn(pred,batch['label'].flatten())\n",
    "            loss_item=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f'loss:{loss_item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1edf531-1542-4deb-85d4-09c7d698d115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:5.442234516143799: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7108/7108 [02:07<00:00, 55.81it/s]\n",
      "loss:5.241026401519775: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7108/7108 [02:12<00:00, 53.48it/s]\n",
      "loss:5.638738632202148: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7108/7108 [02:15<00:00, 52.55it/s]\n",
      "loss:5.652792930603027: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7108/7108 [02:14<00:00, 52.79it/s]\n",
      "loss:5.26169490814209: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7108/7108 [02:15<00:00, 52.49it/s]\n"
     ]
    }
   ],
   "source": [
    "train(epochs,model,loss_fn,optimizer,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00f080f4-5fec-4a0f-81f3-c3a06664cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_the_acc(model,starting_seq,max_seq_len=128):\n",
    "    model=model.to(device)\n",
    "    input_ids=[word2int['<bos>']] +[word2int[word] if word in word2int else word2int['<unk>'] for word in starting_seq.split()]\n",
    "\n",
    "    input_ids=torch.LongTensor(input_ids).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            new_word_dist=model(input_ids[-1].unsqueeze(dim=0))\n",
    "            new_word=new_word_dist.squeeze().argmax()\n",
    "            input_ids=torch.cat([input_ids,new_word.unsqueeze(0)])\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9ba7784-a6f0-4f98-9868-139b977a8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=check_the_acc(model,'This is what I',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c961d443-d381-4f6f-9fee-9cad70a632ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[int2word[i.item()] for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0340eec-4542-4766-b48a-88d74d54210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>', '<unk>', 'is', 'what', '<unk>', '<eos>', 's', 'way', 'to', 'the']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ad3c7-5ae0-4f70-b7d0-e911e6089d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f12047-8838-41cc-bd30-1a51131d7e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b7306-539a-4483-9510-94be70b33872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
